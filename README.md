# ğŸ“„ RAG Document Assistant

**Chat with your documents. 100% local. Your data never leaves your server.**

> Most RAG tutorials send your private documents to OpenAI. This runs entirely on your machine using Ollama. Self-hosted. Air-gapped if you want.

---

## What It Does

Upload documents. Ask questions. Get answers with sources.

```
You: "What's our refund policy?"
Bot: "Based on employee_handbook.pdf (page 12): Refunds are processed 
     within 5-7 business days for all purchases made within 30 days..."
```

---

## Use Cases

| Scenario | Example |
|----------|---------|
| **Internal Wiki** | "How do I submit a PTO request?" |
| **Customer Support** | "What's covered under warranty?" |
| **Sales Enablement** | "Find case studies about healthcare clients" |
| **Legal/Compliance** | "Search all contracts for indemnification clauses" |
| **Onboarding** | "What's the dress code policy?" |

---

## Quick Start

### Prerequisites

- Python 3.10+
- [Ollama](https://ollama.com/) installed
- Docker (optional, for containerized deployment)

### 1. Clone and Install

```bash
git clone https://github.com/ChrisxDamien/rag-document-assistant.git
cd rag-document-assistant
pip install -r requirements.txt
```

### 2. Pull Required Models

```bash
# LLM for chat
ollama pull llama3.2

# Embeddings for vector search
ollama pull nomic-embed-text
```

### 3. Run the App

```bash
streamlit run app/main.py
```

Open http://localhost:8501 in your browser.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      STREAMLIT UI                           â”‚
â”‚  - Upload documents                                         â”‚
â”‚  - Chat interface                                           â”‚
â”‚  - Source citations                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG PIPELINE                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   INGEST     â”‚      â”‚   RETRIEVE   â”‚      â”‚   GENERATE   â”‚
â”‚              â”‚      â”‚              â”‚      â”‚              â”‚
â”‚ â€¢ Load docs  â”‚      â”‚ â€¢ Embed queryâ”‚      â”‚ â€¢ Build promptâ”‚
â”‚ â€¢ Chunk text â”‚      â”‚ â€¢ Search DB  â”‚      â”‚ â€¢ Call LLM   â”‚
â”‚ â€¢ Embed      â”‚      â”‚ â€¢ Rerank     â”‚      â”‚ â€¢ Stream     â”‚
â”‚ â€¢ Store      â”‚      â”‚ â€¢ Return top â”‚      â”‚   response   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚                     â”‚
       â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CHROMADB    â”‚      â”‚   OLLAMA     â”‚      â”‚   OLLAMA     â”‚
â”‚  (Vectors)   â”‚      â”‚ (Embeddings) â”‚      â”‚   (LLM)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Project Structure

```
rag-document-assistant/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # Streamlit interface
â”‚   â””â”€â”€ rag/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ ingest.py        # Document loading + chunking
â”‚       â”œâ”€â”€ embeddings.py    # Vector embedding with Ollama
â”‚       â”œâ”€â”€ retrieval.py     # Semantic search + reranking
â”‚       â””â”€â”€ chat.py          # Conversational RAG chain
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_docs/         # Example documents for demo
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ ARCHITECTURE.md      # Technical deep-dive
â”œâ”€â”€ docker-compose.yml       # One-command deployment
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## Supported File Types

| Type | Extensions |
|------|------------|
| Documents | `.pdf`, `.docx`, `.txt`, `.md` |
| Spreadsheets | `.csv` (coming soon) |
| Web | `.html` (coming soon) |

---

## Configuration

### Using Ollama (Default - Free)

No configuration needed. Ensure Ollama is running:

```bash
ollama serve
```

### Using OpenAI (Optional)

Create a `.env` file:

```bash
cp .env.example .env
# Add your OPENAI_API_KEY
```

---

## Docker Deployment

### Run with Docker Compose

```bash
docker-compose up -d
```

This starts:
- Streamlit app on port 8501
- ChromaDB for vector storage
- Ollama for LLM (if not running externally)

### For Production

```bash
docker-compose -f docker-compose.prod.yml up -d
```

---

## How It Works

### 1. Document Ingestion

```python
from app.rag.ingest import ingest_document

# Load and chunk a document
chunks = ingest_document("company_handbook.pdf")
# Returns: List of text chunks with metadata
```

### 2. Embedding + Storage

```python
from app.rag.embeddings import embed_and_store

# Embed chunks and store in vector DB
embed_and_store(chunks, collection="company_docs")
```

### 3. Retrieval

```python
from app.rag.retrieval import retrieve

# Find relevant chunks
results = retrieve("What's the vacation policy?", collection="company_docs", top_k=5)
```

### 4. Generation

```python
from app.rag.chat import chat

# Generate answer with sources
response = chat("What's the vacation policy?", collection="company_docs")
print(response.answer)
print(response.sources)
```

---

## Key Features

| Feature | Description |
|---------|-------------|
| **100% Local** | No data leaves your machine |
| **Source Citations** | Every answer shows where it came from |
| **Conversational** | Remembers context within a session |
| **Chunk Overlap** | Smart chunking preserves context |
| **Reranking** | Better results than naive similarity |

---

## Roadmap

- [x] PDF ingestion
- [x] Basic RAG pipeline
- [x] Streamlit UI
- [x] Source citations
- [ ] DOCX support
- [ ] CSV/Excel support
- [ ] Multi-collection search
- [ ] Hybrid search (keyword + semantic)
- [ ] Conversation memory persistence
- [ ] API endpoint (FastAPI)

---

## Why Local RAG?

| Concern | Cloud RAG | Local RAG |
|---------|-----------|-----------|
| **Privacy** | Docs sent to third party | Stays on your server |
| **Cost** | Per-token pricing adds up | Free after hardware |
| **Latency** | Network round-trip | Local = fast |
| **Compliance** | May violate data policies | Full control |
| **Availability** | Depends on provider uptime | Runs offline |

---

## Contributing

PRs welcome. Please:

1. Keep it simple - this is meant to be understandable
2. Test with Ollama (free tier must work)
3. Update documentation

---

## License

MIT - Use it however you want.

---

## About

Built by [Chris Damien](https://linkedin.com/in/chris-damien) as part of my work helping businesses leverage AI.

**More resources:**
- [LinkedIn](https://linkedin.com/in/chris-damien) - Weekly AI automation content
- [Other Projects](https://github.com/ChrisxDamien) - More tools

---

*If this saved you time, star the repo â­*
