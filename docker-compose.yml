version: '3.8'

services:
  rag-assistant:
    build: .
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
    environment:
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - OLLAMA_MODEL=llama3.2
      - OLLAMA_EMBED_MODEL=nomic-embed-text
      - CHROMA_PERSIST_DIR=/app/data/chroma
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped

# Note: This assumes Ollama is running on the host machine.
# For a fully containerized setup, add Ollama as a service:
#
#   ollama:
#     image: ollama/ollama
#     ports:
#       - "11434:11434"
#     volumes:
#       - ollama_data:/root/.ollama
#
# volumes:
#   ollama_data:
